[strings]
# Mode : train, test, serve
mode = train
train_data=train_data
seq_data = train_data/seq.data
vocab_inp_path=train_data/inp.vocab
vocab_tar_path=train_data/tar.vocab
#训练集原始文件
resource_data = train_data/xiaohuangji50w_fenciA.conv
#分割后的训练样本文件
split_train_data=train_data/seq_data_
#读取识别原始文件中段落和行头的标示
e = E
m = M
model_data = model_data
log_dir=log_dir
[ints]
# vocabulary size 
# 	20,000 is a reasonable size
vocab_inp_size = 20000
vocab_tar_size = 20000
embedding_dim=128
train_epoch=10
# typical options : 128, 256, 512, 1024
layer_size = 512
batch_size = 64
#句子的最长长度
max_length=20
number_work=2
[floats]
#设置最小Loss，当模型loss值达到这个水平后停止训练
min_loss=0.2

